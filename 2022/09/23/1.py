# torch.nn.EmBedding
# 词嵌入

# 在图像分类问题会使用 one-hot 编码。
# 比如LeNet中一共有10个数字0-9，
# 如果这个数字是2的话类，
# 它的编码就是 (0，0，1，0， 0，0 ，0，0，0，0)，对于分类问题这样表示十分的清楚，
# 但是在自然语言处理中，因为单词的数目过多比如有 10000 个不同的词，
# 那么使用 one-hot 这样的方式来定义，效率就特别低，
# 每个单词都是 10000 维的向量。其中只有一位是 1 ，
# 其余都是 0，特别占用内存，
# 而且也不能体现单词的词性，
# 因为每一个单词都是 one-hot，
# 虽然有些单词在语义上会更加接近.
# 但是 one-hot 没办法体现这个特点，
# 所以 必须使用另外一种方式定义每一个单词。

# 用不同的特征来对各个词汇进行表征，相对与不同的特征，不同的单词均有不同的值这就是词嵌入。
# 词嵌入不仅对不同单词实现了特征化的表示，
# 还能通过计算词与词之间的相似度，
# 实际上是在多维空间中，
# 寻找词向量之间各个维度的距离相似度，
# 我们就可以实现类比推理，比如说夏天和热，冬天和冷，都是有关联关系的。
# 在 PyTorch 中我们用 nn.Embedding 层来做嵌入词袋模型，

# Embedding层
# 第一个输入表示我们有多少个词，(batch_size)
# 第二个输入表示每一个词使用多少维度的向量表示。(input_features)
import torch
embedding = torch.nn.Embedding(num_embeddings=10,
                               embedding_dim=3)
input_tensor = torch.tensor([[1, 2, 4, 5],[4, 3, 2, 9]])
output = embedding(input_tensor)
print(output)
